{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from models import Multi_STGAC, ZINB, ZINB_GNN\n",
    "from utils2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohamedchaaben/Downloads/STZINB-main/utils2.py:65: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1670525473998/work/torch/csrc/utils/tensor_new.cpp:233.)\n",
      "  train_data, train_label = torch.Tensor(train_data), torch.Tensor(train_label)\n"
     ]
    }
   ],
   "source": [
    "main_df = pd.read_pickle(\"/Users/mohamedchaaben/Downloads/STGAC-master/inputs/main_matrix_5_23.pkl\")\n",
    "A = np.load(\"A.npy\")\n",
    "train_loader, valid_loader, test_loader, max_load = PrepareDataset(main_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "n_input = 410\n",
    "n_hidden = 410\n",
    "p_dropout = 0.5\n",
    "lr = 1e-5\n",
    "n_epochs = 10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "zinb_gnn_model = ZINB_GNN(Multi_STGAC(A), ZINB, n_input, n_hidden, p_dropout)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(zinb_gnn_model.parameters(), lr=lr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohamedchaaben/opt/anaconda3/envs/torch-env/lib/python3.9/site-packages/torch/nn/modules/conv.py:459: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1670525473998/work/aten/src/ATen/native/Convolution.cpp:896.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "[W NNPACK.cpp:53] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 34\u001B[0m\n\u001B[1;32m     29\u001B[0m BNP_W \u001B[38;5;241m=\u001B[39m BNP_W\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     32\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 34\u001B[0m n, p, pi \u001B[38;5;241m=\u001B[39m \u001B[43mzinb_gnn_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs_R\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs_D\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs_W\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mBNP_R\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mBNP_D\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mBNP_W\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     35\u001B[0m loss \u001B[38;5;241m=\u001B[39m zinb_gnn_model\u001B[38;5;241m.\u001B[39mzinb_loss(labels_R, n, p, pi)\n\u001B[1;32m     36\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Downloads/STZINB-main/models.py:307\u001B[0m, in \u001B[0;36mZINB_GNN.forward\u001B[0;34m(self, x1, x2, x3, bnp1, bnp2, bnp3)\u001B[0m\n\u001B[1;32m    306\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x1, x2, x3, bnp1, bnp2, bnp3):\n\u001B[0;32m--> 307\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgnn_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx3\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbnp1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbnp2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbnp3\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    308\u001B[0m     n, p, pi \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mzinb_model(x)\n\u001B[1;32m    309\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m n, p, pi\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Downloads/STZINB-main/models.py:257\u001B[0m, in \u001B[0;36mMulti_STGAC.forward\u001B[0;34m(self, Xr, Xd, Xw, BNPr, BNPd, BNPw)\u001B[0m\n\u001B[1;32m    256\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, Xr, Xd, Xw, BNPr, BNPd, BNPw):\n\u001B[0;32m--> 257\u001B[0m     X1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mRecent_component\u001B[49m\u001B[43m(\u001B[49m\u001B[43mXr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mBNPr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    258\u001B[0m     X2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mDaily_component(Xd, BNPd)\n\u001B[1;32m    259\u001B[0m     X3 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mWeekly_component(Xw, BNPw)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Downloads/STZINB-main/models.py:230\u001B[0m, in \u001B[0;36mCOMPONENT.forward\u001B[0;34m(self, x, BNP)\u001B[0m\n\u001B[1;32m    228\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, BNP):\n\u001B[1;32m    229\u001B[0m     x, x11, x12 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mTSM1(x, BNP)\n\u001B[0;32m--> 230\u001B[0m     x, x21, x22 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTSM2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mBNP\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    231\u001B[0m     x, x31, x32 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mTSM3(x, BNP)\n\u001B[1;32m    232\u001B[0m     x, x41, x42 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mTSM4(x, BNP)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Downloads/STZINB-main/models.py:200\u001B[0m, in \u001B[0;36mTSM_others.forward\u001B[0;34m(self, x, BNP)\u001B[0m\n\u001B[1;32m    198\u001B[0m x2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mGDCN2(x)\n\u001B[1;32m    199\u001B[0m x_middle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgat(x2, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mA)\n\u001B[0;32m--> 200\u001B[0m x_middle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mGCN2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_middle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mBNP\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    201\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mBN2(x\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m) \u001B[38;5;241m+\u001B[39m x_middle\u001B[38;5;241m.\u001B[39mfloat())\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m    203\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x, x1, x2\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Downloads/STZINB-main/models.py:133\u001B[0m, in \u001B[0;36mGCN.forward\u001B[0;34m(self, input, BNP)\u001B[0m\n\u001B[1;32m    130\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, BNP):\n\u001B[1;32m    131\u001B[0m     \u001B[38;5;66;03m# some squeeze and unsqueeze\u001B[39;00m\n\u001B[1;32m    132\u001B[0m     x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39meinsum(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mij,xyij -> xyij\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mA, torch\u001B[38;5;241m.\u001B[39mTensor(BNP)\u001B[38;5;241m.\u001B[39mdouble())\n\u001B[0;32m--> 133\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    134\u001B[0m     x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m410\u001B[39m, \u001B[38;5;241m410\u001B[39m)\n\u001B[1;32m    135\u001B[0m     x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39meinsum(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mij,ijk->ik\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mdouble()\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m), x\u001B[38;5;241m.\u001B[39mdouble())\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Downloads/STZINB-main/models.py:114\u001B[0m, in \u001B[0;36mFilterLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfilter_square_matrix\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmatmul\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdouble\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# define data loading and preprocessing steps\n",
    "train_loader = train_loader\n",
    "val_loader = valid_loader\n",
    "\n",
    "# training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # set model to training mode\n",
    "    zinb_gnn_model.train()\n",
    "\n",
    "    # iterate over batches of training data\n",
    "    for data in train_loader:\n",
    "        inputs_R, labels_R = data[0]\n",
    "        inputs_D, labels_D = data[1]\n",
    "        inputs_W, labels_W = data[2]\n",
    "\n",
    "        BNP_R, BNP_D, BNP_W = create_BNP(inputs_R), create_BNP(inputs_D), create_BNP(inputs_W)\n",
    "        BNP_R, BNP_D, BNP_W = Variable(BNP_R), Variable(BNP_D), Variable(BNP_W)\n",
    "\n",
    "\n",
    "        inputs_R, labels_R = Variable(inputs_R[:,:,1:]), Variable(labels_R[:,:,1:])\n",
    "        inputs_D, labels_D = Variable(inputs_D[:,:,1:]), Variable(labels_D[:,:,1:])\n",
    "        inputs_W, labels_W = Variable(inputs_W[:,:,1:]), Variable(labels_W[:,:,1:])\n",
    "\n",
    "        inputs_R = inputs_R.to(device)\n",
    "        inputs_D = inputs_D.to(device)\n",
    "        inputs_W = inputs_W.to(device)\n",
    "        BNP_R = BNP_R.to(device)\n",
    "        BNP_D = BNP_D.to(device)\n",
    "        BNP_W = BNP_W.to(device)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        n, p, pi = zinb_gnn_model(inputs_R, inputs_D, inputs_W, BNP_R, BNP_D, BNP_W)\n",
    "        loss = zinb_gnn_model.zinb_loss(labels_R, n, p, pi)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}: training loss = {loss:.3f}\")\n",
    "\n",
    "    # set model to evaluation mode and compute validation loss\n",
    "    zinb_gnn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for val_data in val_loader:\n",
    "            inputs_val_R, labels_val_R = val_data[0]\n",
    "            inputs_val_D, labels_val_D = val_data[1]\n",
    "            inputs_val_W, labels_val_W = val_data[2]\n",
    "\n",
    "\n",
    "            BNP_val_R, BNP_val_D, BNP_val_W = create_BNP(inputs_val_R), create_BNP(inputs_val_D), create_BNP(inputs_val_W)\n",
    "            BNP_val_R, BNP_val_D, BNP_val_W = Variable(BNP_val_R), Variable(BNP_val_D), Variable(BNP_val_W)\n",
    "\n",
    "            inputs_val_R, labels_val_R = Variable(inputs_val_R[:,:,1:]), Variable(labels_val_R[:,:,1:])\n",
    "            inputs_val_D, labels_val_D = Variable(inputs_val_D[:,:,1:]), Variable(labels_val_D[:,:,1:])\n",
    "            inputs_val_W, labels_val_W = Variable(inputs_val_W[:,:,1:]), Variable(labels_val_W[:,:,1:])\n",
    "\n",
    "            inputs_val_R = inputs_val_R.to(device)\n",
    "            inputs_val_D = inputs_val_D.to(device)\n",
    "            inputs_val_W = inputs_val_W.to(device)\n",
    "            BNP_val_R = BNP_val_R.to(device)\n",
    "            BNP_val_D = BNP_val_D.to(device)\n",
    "            BNP_val_W = BNP_val_W.to(device)\n",
    "\n",
    "            n, p, pi = zinb_gnn_model(inputs_val_R, inputs_val_D, inputs_val_W, BNP_val_R, BNP_val_D, BNP_val_W)\n",
    "            val_loss += zinb_gnn_model.zinb_loss(labels_val_R, n, p, pi)\n",
    "\n",
    "            valid_pred_flow = n * p * (1-pi)\n",
    "            print(valid_pred_flow.shape)\n",
    "            print(labels_val_R.shape)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "    # print epoch number and validation loss\n",
    "    print(f\"Epoch {epoch+1}: validation loss = {val_loss:.3f}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
